FROM apache/airflow:2.8.0
USER root

# Instala dependências de sistema
RUN apt-get update && apt-get install -y \
    openjdk-17-jdk \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Configura JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

# --- Seção para a instalação do Spark ---
# Definição das versões
ENV SPARK_VERSION=3.5.0
ENV HADOOP_VERSION=3
ENV ICEBERG_VERSION=1.4.2

# Download e instalação do Spark
RUN curl -O https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark \
    && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Download e instalação dos conectores Iceberg e S3
# [cite_start]O arquivo `Dockerfile.spark` original fornecido contém a fonte para esses downloads [cite: 1]
RUN curl -L https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/${ICEBERG_VERSION}/iceberg-spark-runtime-3.5_2.12-${ICEBERG_VERSION}.jar \
    -o /opt/spark/jars/iceberg-spark-runtime-3.5_2.12-${ICEBERG_VERSION}.jar \
 && curl -L https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar \
    -o /opt/spark/jars/hadoop-aws-3.3.4.jar \
 && curl -L https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar \
    -o /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar

# Define as variáveis de ambiente para o Spark
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
# --- Fim da seção do Spark ---

USER airflow

# Versões do ambiente
ARG AIRFLOW_VERSION=2.8.0
ARG PYTHON_MAJOR_MINOR=3.8   # use 3.10 se a imagem base for python3.10

# Dependências Python do projeto
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir \
  -c https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_MAJOR_MINOR}.txt \
  -r /requirements.txt

# Suporte à API (FastAPI + auth_manager novo)
RUN pip install --no-cache-dir "apache-airflow[api]==${AIRFLOW_VERSION}"

# Projeto
COPY --chown=airflow:root dags/     /opt/airflow/dags/
COPY --chown=airflow:root plugins/  /opt/airflow/plugins/
COPY --chown=airflow:root src/      /opt/airflow/src/
ENV PYTHONPATH="/opt/airflow/src:${PYTHONPATH}"

# (opcional, se for rodar pytest dentro do container)
COPY --chown=airflow:root tests/    /opt/airflow/tests/

# (opcional) config específica
# COPY --chown=airflow:root config/airflow.cfg /opt/airflow/airflow.cfg
